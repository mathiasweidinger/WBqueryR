tfidf.matrix <- scale(tfidf.matrix, center = FALSE,
scale = sqrt(colSums(tfidf.matrix^2)))
tfidf.matrix[0:3, ]
query.vector <- tfidf.matrix[, (N.labels + 1)]
tfidf.matrix <- tfidf.matrix[, 1:N.labels]
doc.scores <- t(query.vector) %*% tfidf.matrix
results.df <- data.frame(doc = names(labels), score = t(labels),
text = unlist(labels))
length(labels)
length(t(labels))
results.df <- data.frame(doc = names(labels), score = t(labels),
text = unlist(labels))
results.df <- tibble(doc = names(labels), score = t(labels),
text = unlist(labels))
item_vars$ALB_2012_LSMS_v01_M_v01_A_PUF%$% labl -> labels
names(labels) <- item_vars$ALB_2012_LSMS_v01_M_v01_A_PUF$name
N.labels <- length(labels)
query <- "main occupation"
my.docs <- VectorSource(c(labels, query))
my.docs$Names <- c(names(labels), "query")
my.corpus <- Corpus(my.docs)
my.corpus
#remove punctuation
my.corpus <- tm_map(my.corpus, removePunctuation)
#remove numbers, uppercase, additional spaces
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, stripWhitespace)
#create document matrix in a format that is efficient
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus)
colnames(term.doc.matrix.stm) <- c(names(labels), "query")
inspect(term.doc.matrix.stm[0:14, ])
#compare number of bytes of normal matrix against triple matrix format
term.doc.matrix <- as.matrix(term.doc.matrix.stm)
cat("Dense matrix representation costs", object.size(term.doc.matrix), "bytes.\n",
"Simple triplet matrix representation costs", object.size(term.doc.matrix.stm),
"bytes.")
#constructing the Vector Space Model
get.tf.idf.weights <- function(tf.vec) {
# Compute tfidf weights from term frequency vector
n.docs <- length(tf.vec)
doc.frequency <- length(tf.vec[tf.vec > 0])
weights <- rep(0, length(tf.vec))
weights[tf.vec > 0] <- (1 + log2(tf.vec[tf.vec > 0])) * log2(n.docs/doc.frequency)
return(weights)
}
tfidf.matrix <- t(apply(term.doc.matrix, 1,
FUN = function(row) {get.tf.idf.weights(row)}))
colnames(tfidf.matrix) <- colnames(term.doc.matrix)
tfidf.matrix[0:3, ]
tfidf.matrix <- scale(tfidf.matrix, center = FALSE,
scale = sqrt(colSums(tfidf.matrix^2)))
tfidf.matrix[0:3, ]
query.vector <- tfidf.matrix[, (N.labels + 1)]
tfidf.matrix <- tfidf.matrix[, 1:N.labels]
doc.scores <- t(query.vector) %*% tfidf.matrix
View(doc.scores)
View(doc.scores)
results.df <- tibble(doc = names(labels), score = t(doc.scores),text = unlist(labels))
results.df <- tibble(doc = names(labels), score = t(doc.scores),text = labels)
results.df <- results.df[order(results.df$score, decreasing = TRUE), ]
View(results.df)
#print scores of documents by the query
results<- results.df[,c(1,2)]
print(results, row.names = FALSE, right = FALSE, digits = 2)
#print scores of documents by the query
results<- results.df[,c(1,3)]
print(results, row.names = FALSE, right = FALSE, digits = 2)
#print scores of documents by the query
results<- results.df[,c(1:3)]
print(results, row.names = FALSE, right = FALSE, digits = 2)
View(results)
View(labels)
View(results)
View(results.df)
View(results)
View(results.df)
View(results.df)
results %<>% filter(score >= 0.5)
View(results)
vsm_score <- function(df, query, accuracy = 0.5){
require(tm)
require(dplyr)
df %$% labl -> labels
names(labels) <- df$name
N.labels <- length(labels)
query <- "main occupation"
my.docs <- VectorSource(c(labels, query))
my.docs$Names <- c(names(labels), "query")
my.corpus <- Corpus(my.docs)
my.corpus
#remove punctuation
my.corpus <- tm_map(my.corpus, removePunctuation)
#remove numbers, uppercase, additional spaces
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, stripWhitespace)
#create document matrix in a format that is efficient
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus)
colnames(term.doc.matrix.stm) <- c(names(labels), "query")
#compare number of bytes of normal matrix against triple matrix format
term.doc.matrix <- as.matrix(term.doc.matrix.stm)
cat("Dense matrix representation costs", object.size(term.doc.matrix), "bytes.\n",
"Simple triplet matrix representation costs", object.size(term.doc.matrix.stm),
"bytes.")
#constructing the Vector Space Model
get.tf.idf.weights <- function(tf.vec) {
# Compute tfidf weights from term frequency vector
n.docs <- length(tf.vec)
doc.frequency <- length(tf.vec[tf.vec > 0])
weights <- rep(0, length(tf.vec))
weights[tf.vec > 0] <- (1 + log2(tf.vec[tf.vec > 0])) * log2(n.docs/doc.frequency)
return(weights)
}
tfidf.matrix <- t(apply(term.doc.matrix, 1,
FUN = function(row) {get.tf.idf.weights(row)}))
colnames(tfidf.matrix) <- colnames(term.doc.matrix)
tfidf.matrix[0:3, ]
tfidf.matrix <- scale(tfidf.matrix, center = FALSE,
scale = sqrt(colSums(tfidf.matrix^2)))
tfidf.matrix[0:3, ]
query.vector <- tfidf.matrix[, (N.labels + 1)]
tfidf.matrix <- tfidf.matrix[, 1:N.labels]
doc.scores <- t(query.vector) %*% tfidf.matrix
results.df <- tibble(doc = names(labels), score = t(doc.scores),text = labels)
results.df <- results.df[order(results.df$score, decreasing = TRUE), ]
# filter out
results.df %>% filter(score >= 0.5) -> scores
return(scores)
}
key = c("per capita consumption")
collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(SnowballC)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
gc()
key = c("per capita consumption")
collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(SnowballC)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
key = c("per capita consumption")
collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble %$% # build tibble
idno -> items # extract item names
message("gathering codebooks; this might take a while...")
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
source("R/vsm_score.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # set parallel clusters, end of parallelized task
vsm_score(df = item_vars$ALB_2012_LSMS_v01_M_v01_A_PUF, query = key) -> TESTYFESTY
View(TESTYFESTY)
item_vars %>% lapply(. %>% vsm_score(. , query = key)) -> TESTYFESTY
View(TESTYFESTY)
view(TESTYFESTY$`BIH_2001-2004_LSMS_v01_M_v01_A_EPCT`)
View(TESTYFESTY)
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "lsms",# collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order = ""      # c("asc","desc")
){
key = c("per capita consumption")
collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble %$% # build tibble
idno -> items # extract item names
message("gathering codebooks; this might take a while...")
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
source("R/vsm_score.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # set parallel clusters, end of parallelized task
item_vars %>% lapply(. %>% vsm_score(. , query = key)) -> score
key = c("per capita consumption")
collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble %$% # build tibble
idno -> items # extract item names
message("gathering codebooks; this might take a while...")
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
source("R/vsm_score.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # set parallel clusters, end of parallelized task
item_vars %>% lapply(. %>% vsm_score(. , query = key)) -> score
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "lsms",# collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order = ""      # c("asc","desc")
){
key = c("per capita consumption")
collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble %$% # build tibble
idno -> items # extract item names
message("gathering codebooks; this might take a while...")
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
source("R/vsm_score.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # set parallel clusters, end of parallelized task
item_vars %>% lapply(. %>% vsm_score(. , query = key)) -> score
#------------------------------------------------------------------------------------------------
stopCluster(cl) # switch off clusters
wanna_read <- readline(  # ask whether to print or not
prompt = "Done! Should I print the results? (type y for YES, n for NO):"
)
if (grepl(pattern = "y", wanna_read, ignore.case = TRUE) == TRUE){
# if yes...
message("RESULTS:")
for (i in seq(1:length(item_vars))){ # ...print the name of each dataset
message("")
message(paste0(names(item_vars)[i], " contains the variable(s):"))
#...followed by each variable in it that matches search criteria
for (v in 1:length(item_vars[[i]]$labl)){
message(paste0("     - ",
item_vars[[i]]$name[v],
" (", item_vars[[i]]$labl[v],")"))
}
}
}
else{
Sys.sleep(time = 1) # else do nothing
}
return(item_vars) # give output
} # end of WBquery()
