data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble %$% # build tibble
idno -> items # extract item names
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
source("R/vsm_score.R")
})
# download codebooks
message("gathering codebooks...")
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # set parallel clusters, end of parallelized task
message("scoring for key words...")
item_vars %>% pblapply(. %>% vsm_score(. , query = key), cl = cl) %>%
keep(., ~nrow(.) > 0) -> scores # drop empty tibbles
stopCluster(cl) # switch off clusters
wanna_read <- readline(  # ask whether to print or not
prompt = "Done! Should I print the results? (type y for YES, n for NO):"
)
if (grepl(pattern = "y", wanna_read, ignore.case = TRUE) == TRUE){
# if yes...
message("RESULTS:")
for (i in seq(1:length(scores))){ # ...print the name of each dataset
message("")
message(paste0(names(scores)[i], " contains the variable(s):"))
#...followed by each variable in it that matches search criteria
for (v in 1:length(scores[[i]]$text)){
message(paste0("    ",round(scores[[i]]$score[v], digits = 2)," - ",
scores[[i]]$doc[v],
" (", scores[[i]]$text[v],")"))
}
}
}
else{
Sys.sleep(time = 1) # else do nothing
}
#return(item_vars) # give output
}
item_vars <- WBquery(key = c("per capita consumption"))
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "lsms",# collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order = ""      # c("asc","desc")
){
# key = c("per capita consumption")
# collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble %$% # build tibble
idno -> items # extract item names
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
source("R/vsm_score.R")
})
# download codebooks
message("gathering codebooks...")
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # set parallel clusters, end of parallelized task
message("scoring for key words...")
item_vars %>% pblapply(. %>% vsm_score(. , query = key), cl = cl) %>%
keep(., ~nrow(.) > 0) -> scores # drop empty tibbles
stopCluster(cl) # switch off clusters
wanna_read <- readline(  # ask whether to print or not
prompt = "Done! Should I print the results? (type y for YES, n for NO):"
)
if (grepl(pattern = "y", wanna_read, ignore.case = TRUE) == TRUE){
# if yes...
message("RESULTS:")
for (i in seq(1:length(scores))){ # ...print the name of each dataset
message("")
message(paste0(names(scores)[i], " contains the variable(s):"))
#...followed by each variable in it that matches search criteria
for (v in 1:length(scores[[i]]$text)){
message(paste0("    ",round(scores[[i]]$score[v], digits = 2)," - ",
scores[[i]]$doc[v],
" (", scores[[i]]$text[v],")"))
}
}
}
else{
Sys.sleep(time = 1) # else do nothing
}
#return(item_vars) # give output
} # end of WBquery()
item_vars <- WBquery(key = c("per capita consumption"))
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "lsms",# collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order = "",    # c("asc","desc")
accuracy = 0.5      # set search accuracy (vsm score limit)
){
# key = c("per capita consumption")
# collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble %$% # build tibble
idno -> items # extract item names
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
source("R/vsm_score.R")
})
# download codebooks
message("gathering codebooks...")
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # set parallel clusters, end of parallelized task
message("scoring for key words...")
item_vars %>% pblapply(. %>% vsm_score(. , query = key, accuracy = accuracy), cl = cl) %>%
keep(., ~nrow(.) > 0) -> scores # drop empty tibbles
stopCluster(cl) # switch off clusters
wanna_read <- readline(  # ask whether to print or not
prompt = "Done! Should I print the results? (type y for YES, n for NO):"
)
if (grepl(pattern = "y", wanna_read, ignore.case = TRUE) == TRUE){
# if yes...
message("RESULTS:")
for (i in seq(1:length(scores))){ # ...print the name of each dataset
message("")
message(paste0(names(scores)[i], " contains the variable(s):"))
#...followed by each variable in it that matches search criteria
for (v in 1:length(scores[[i]]$text)){
message(paste0("    ",round(scores[[i]]$score[v], digits = 2)," - ",
scores[[i]]$doc[v],
" (", scores[[i]]$text[v],")"))
}
}
}
else{
Sys.sleep(time = 1) # else do nothing
}
#return(item_vars) # give output
} # end of WBquery()
item_vars <- WBquery(key = c("per capita consumption"),
country = c("malawi", "nigeria"),
accuracy = 0.85)
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "lsms",# collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order = "",    # c("asc","desc")
accuracy = 0.5      # set search accuracy (vsm score limit)
){
# key = c("per capita consumption")
# collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble %$% # build tibble
idno -> items # extract item names
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
source("R/vsm_score.R")
})
# download codebooks
message("gathering codebooks...")
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # set parallel clusters, end of parallelized task
message("scoring for key words...")
item_vars %>% pblapply(. %>% vsm_score(. , query = key, accuracy = accuracy), cl = cl) %>%
keep(., ~nrow(.) > 0) -> scores # drop empty tibbles
stopCluster(cl) # switch off clusters
wanna_read <- readline(  # ask whether to print or not
prompt = "Done! Should I print the results? (type y for YES, n for NO):"
)
if (grepl(pattern = "y", wanna_read, ignore.case = TRUE) == TRUE){
# if yes...
message("RESULTS:")
for (i in seq(1:length(scores))){ # ...print the name of each dataset
message("")
message(paste0(names(scores)[i], " contains the variable(s):"))
#...followed by each variable in it that matches search criteria
for (v in 1:length(scores[[i]]$text)){
message(paste0("    ",round(scores[[i]]$score[v], digits = 2)," - ",
scores[[i]]$doc[v],
" (", scores[[i]]$text[v],")"))
}
}
}
else{
Sys.sleep(time = 1) # else do nothing
}
#return(item_vars) # give output
} # end of WBquery()
item_vars <- WBquery(key = c("per capita consumption"),
country = c("malawi", "nigeria"),
accuracy = 0.85)
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "lsms",# collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order = "",    # c("asc","desc")
accuracy = 0.5      # set search accuracy (vsm score limit)
){
# key = c("per capita consumption")
# collection = c("lsms")
# Load the package required to read JSON files.
require(rjson)
require(tm)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
source("R/vsm_score.R") # source custom scoring function
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble %$% # build tibble
idno -> items # extract item names
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
source("R/vsm_score.R")
})
# download codebooks
message("gathering codebooks...")
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # set parallel clusters, end of parallelized task
message("scoring for key words...")
item_vars %>% pblapply(. %>% vsm_score(. , query = key, accuracy = accuracy), cl = cl) %>%
keep(., ~nrow(.) > 0) -> scores # drop empty tibbles
stopCluster(cl) # switch off clusters
wanna_read <- readline(  # ask whether to print or not
prompt = "Done! Should I print the results? (type y for YES, n for NO):"
)
if (grepl(pattern = "y", wanna_read, ignore.case = TRUE) == TRUE){
# if yes...
message("RESULTS:")
for (i in seq(1:length(scores))){ # ...print the name of each dataset
message("")
message(paste0(names(scores)[i], " contains the variable(s):"))
#...followed by each variable in it that matches search criteria
for (v in 1:length(scores[[i]]$text)){
message(paste0("    ",round(scores[[i]]$score[v], digits = 2)," - ",
scores[[i]]$doc[v],
" (", scores[[i]]$text[v],")"))
}
}
}
else{
Sys.sleep(time = 1) # else do nothing
}
#return(item_vars) # give output
}
item_vars <- WBquery(key = c("per capita consumption"),
country = c("malawi", "nigeria"),
accuracy = 0.85)
