query.vector <- tfidf.matrix[, (N.labels + 1)]
tfidf.matrix <- tfidf.matrix[, 1:N.labels]
doc.scores <- t(query.vector) %*% tfidf.matrix
results.df <- tibble(doc = names(labels), score = t(doc.scores),labl = labels)
results.df <- results.df[order(results.df$score, decreasing = TRUE), ]
results.df %>% filter(score >= accuracy) -> scores    # filter out
return(scores)
}
# Process search criteria
# key <- ifelse(exists("key"),
#               paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- httr::GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
httr::status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble -> items # build tibble
# # set number of active cores for parallelization
# parallel::detectCores() %>% parallel::makeCluster() -> cl # detect cores and make clusters
#
# # prep each cluster
# parallel::clusterEvalQ(cl, {
#
#     # Define multiple join function.
#
#     multi_join <- function(list_of_loaded_data, join_func){
#         output <- Reduce(function(x, y) {join_func(x, y)}, list_of_loaded_data)
#         return(output)
#     }
#
#     # Define vsm_score function.
#
#     vsm_score <- function(df, query, accuracy = 0.5){
#
#         df %$% labl -> labels
#         names(labels) <- df$name
#         N.labels <- length(labels)
#
#         my.docs <- tm::VectorSource(c(labels, query))
#         my.docs$Names <- c(names(labels), "query")
#
#         my.corpus <- tm::Corpus(my.docs)
#         my.corpus
#
#         #remove punctuation
#         my.corpus <- tm::tm_map(my.corpus, removePunctuation)
#
#         #remove numbers, uppercase, additional spaces
#         my.corpus <- tm::tm_map(my.corpus, tm::removeNumbers)
#         my.corpus <- tm::tm_map(my.corpus, tm::content_transformer(tolower))
#         my.corpus <- tm::tm_map(my.corpus, tm::stripWhitespace)
#
#         #create document matrix in a format that is efficient
#         term.doc.matrix.stm <- tm::TermDocumentMatrix(my.corpus)
#         colnames(term.doc.matrix.stm) <- c(names(labels), "query")
#
#         #compare number of bytes of normal matrix against triple matrix format
#         term.doc.matrix <- as.matrix(term.doc.matrix.stm)
#
#         #constructing the Vector Space Model
#         get.tf.idf.weights <- function(tf.vec) {
#             # Compute tfidf weights from term frequency vector
#             n.docs <- length(tf.vec)
#             doc.frequency <- length(tf.vec[tf.vec > 0])
#             weights <- rep(0, length(tf.vec))
#             weights[tf.vec > 0] <- (1 + log2(tf.vec[tf.vec > 0])) * log2(n.docs/doc.frequency)
#             return(weights)
#         }
#
#         tfidf.matrix <- t(apply(term.doc.matrix, 1,
#                                 FUN = function(row) {get.tf.idf.weights(row)}))
#         colnames(tfidf.matrix) <- colnames(term.doc.matrix)
#
#         tfidf.matrix <- scale(tfidf.matrix, center = FALSE,
#                               scale = sqrt(colSums(tfidf.matrix^2)))
#
#         query.vector <- tfidf.matrix[, (N.labels + 1)]
#         tfidf.matrix <- tfidf.matrix[, 1:N.labels]
#
#         doc.scores <- t(query.vector) %*% tfidf.matrix
#
#         results.df <- tibble(doc = names(labels), score = t(doc.scores),text = labels)
#
#         results.df <- results.df[order(results.df$score, decreasing = TRUE), ]
#
#
#         results.df %>% filter(score >= accuracy) -> scores    # filter out
#
#         return(scores)
#     }
#
#     })
# download codebooks
message("gathering codebooks...")
item_vars <- items$idno
names(item_vars) <- items$idno
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble)#####, cl = cl) # set parallel clusters, end of task
scores <- vector("list", length(key)) # initiate list of results
names(scores) <- key # assign keywords to item names in results
for(k in 1:length(key)){
message(paste0("scoring for key word ",k, "/", length(key),
" (", key[k],")..."))
item_vars %>% pbapply::pblapply(. %>% vsm_score(. , query = key[k],
accuracy = accuracy)#,cl = cl
) %>%
purrr::keep(., ~nrow(.) > 0) -> scores[[k]] # drop empty tibbles
}
View(scores)
key = c("longitude", "latitude")
country = c("nigeria")
accuracy = 0.5      # set search accuracy (vsm score limit)
collection = "lsms"# collection id
require(rjson) # to read JSON files
require(tm) # text mining
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
require(parallel) # distributed (parallel) computing
#
# # Define multiple join function.
multi_join <- function(list_of_loaded_data, join_func){
output <- Reduce(function(x, y) {join_func(x, y)}, list_of_loaded_data)
return(output)
}
# Define vsm_score function.
vsm_score <- function(df, query, accuracy = 0.5){
# require(tm)
# require(dplyr)
df %$% labl -> labels
names(labels) <- df$name
N.labels <- length(labels)
my.docs <- tm::VectorSource(c(labels, query))
my.docs$Names <- c(names(labels), "query")
my.corpus <- tm::Corpus(my.docs)
my.corpus
#remove punctuation
my.corpus <- tm::tm_map(my.corpus, removePunctuation)
#remove numbers, uppercase, additional spaces
my.corpus <- tm::tm_map(my.corpus, tm::removeNumbers)
my.corpus <- tm::tm_map(my.corpus, tm::content_transformer(tolower))
my.corpus <- tm::tm_map(my.corpus, tm::stripWhitespace)
#create document matrix in a format that is efficient
term.doc.matrix.stm <- tm::TermDocumentMatrix(my.corpus)
colnames(term.doc.matrix.stm) <- c(names(labels), "query")
#compare number of bytes of normal matrix against triple matrix format
term.doc.matrix <- as.matrix(term.doc.matrix.stm)
#constructing the Vector Space Model
get.tf.idf.weights <- function(tf.vec) {
# Compute tfidf weights from term frequency vector
n.docs <- length(tf.vec)
doc.frequency <- length(tf.vec[tf.vec > 0])
weights <- rep(0, length(tf.vec))
weights[tf.vec > 0] <- (1 + log2(tf.vec[tf.vec > 0])) * log2(n.docs/doc.frequency)
return(weights)
}
tfidf.matrix <- t(apply(term.doc.matrix, 1,
FUN = function(row) {get.tf.idf.weights(row)}))
colnames(tfidf.matrix) <- colnames(term.doc.matrix)
tfidf.matrix <- scale(tfidf.matrix, center = FALSE,
scale = sqrt(colSums(tfidf.matrix^2)))
query.vector <- tfidf.matrix[, (N.labels + 1)]
tfidf.matrix <- tfidf.matrix[, 1:N.labels]
doc.scores <- t(query.vector) %*% tfidf.matrix
results.df <- tibble(var = names(labels), score = t(doc.scores),labl = labels)
results.df <- results.df[order(results.df$score, decreasing = TRUE), ]
results.df %>% filter(score >= accuracy) -> scores    # filter out
return(scores)
}
# Process search criteria
# key <- ifelse(exists("key"),
#               paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
request <- httr::GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
httr::status_code(request)
# reshape to get content
text <- httr::content(request, "text", encoding="UTF-8")
data <- jsonlite::fromJSON(text)
data %$% result %$% rows %>% tibble -> items # build tibble
# # set number of active cores for parallelization
# parallel::detectCores() %>% parallel::makeCluster() -> cl # detect cores and make clusters
#
# # prep each cluster
# parallel::clusterEvalQ(cl, {
#
#     # Define multiple join function.
#
#     multi_join <- function(list_of_loaded_data, join_func){
#         output <- Reduce(function(x, y) {join_func(x, y)}, list_of_loaded_data)
#         return(output)
#     }
#
#     # Define vsm_score function.
#
#     vsm_score <- function(df, query, accuracy = 0.5){
#
#         df %$% labl -> labels
#         names(labels) <- df$name
#         N.labels <- length(labels)
#
#         my.docs <- tm::VectorSource(c(labels, query))
#         my.docs$Names <- c(names(labels), "query")
#
#         my.corpus <- tm::Corpus(my.docs)
#         my.corpus
#
#         #remove punctuation
#         my.corpus <- tm::tm_map(my.corpus, removePunctuation)
#
#         #remove numbers, uppercase, additional spaces
#         my.corpus <- tm::tm_map(my.corpus, tm::removeNumbers)
#         my.corpus <- tm::tm_map(my.corpus, tm::content_transformer(tolower))
#         my.corpus <- tm::tm_map(my.corpus, tm::stripWhitespace)
#
#         #create document matrix in a format that is efficient
#         term.doc.matrix.stm <- tm::TermDocumentMatrix(my.corpus)
#         colnames(term.doc.matrix.stm) <- c(names(labels), "query")
#
#         #compare number of bytes of normal matrix against triple matrix format
#         term.doc.matrix <- as.matrix(term.doc.matrix.stm)
#
#         #constructing the Vector Space Model
#         get.tf.idf.weights <- function(tf.vec) {
#             # Compute tfidf weights from term frequency vector
#             n.docs <- length(tf.vec)
#             doc.frequency <- length(tf.vec[tf.vec > 0])
#             weights <- rep(0, length(tf.vec))
#             weights[tf.vec > 0] <- (1 + log2(tf.vec[tf.vec > 0])) * log2(n.docs/doc.frequency)
#             return(weights)
#         }
#
#         tfidf.matrix <- t(apply(term.doc.matrix, 1,
#                                 FUN = function(row) {get.tf.idf.weights(row)}))
#         colnames(tfidf.matrix) <- colnames(term.doc.matrix)
#
#         tfidf.matrix <- scale(tfidf.matrix, center = FALSE,
#                               scale = sqrt(colSums(tfidf.matrix^2)))
#
#         query.vector <- tfidf.matrix[, (N.labels + 1)]
#         tfidf.matrix <- tfidf.matrix[, 1:N.labels]
#
#         doc.scores <- t(query.vector) %*% tfidf.matrix
#
#         results.df <- tibble(doc = names(labels), score = t(doc.scores),text = labels)
#
#         results.df <- results.df[order(results.df$score, decreasing = TRUE), ]
#
#
#         results.df %>% filter(score >= accuracy) -> scores    # filter out
#
#         return(scores)
#     }
#
#     })
# download codebooks
message("gathering codebooks...")
item_vars <- items$idno
names(item_vars) <- items$idno
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble)#####, cl = cl) # set parallel clusters, end of task
scores <- vector("list", length(key)) # initiate list of results
names(scores) <- key # assign keywords to item names in results
for(k in 1:length(key)){
message(paste0("scoring for key word ",k, "/", length(key),
" (", key[k],")..."))
item_vars %>% pbapply::pblapply(. %>% vsm_score(. , query = key[k],
accuracy = accuracy)#,cl = cl
) %>%
purrr::keep(., ~nrow(.) > 0) -> scores[[k]] # drop empty tibbles
}
View(scores)
for(k in 1:length(key)){
message(paste0("scoring for key word ",k, "/", length(key),
" (", key[k],")..."))
item_vars %>% pbapply::pblapply(. %>% vsm_score(. , query = key[k],
accuracy = accuracy)#,cl = cl
) #%>%
#purrr::keep(., ~nrow(.) > 0) -> scores[[k]] # drop empty tibbles
}
View(scores)
for(k in 1:length(key)){
message(paste0("scoring for key word ",k, "/", length(key),
" (", key[k],")..."))
item_vars %>% pbapply::pblapply(. %>% vsm_score(. , query = key[k],
accuracy = accuracy)#,cl = cl
) %>%
purrr::keep(., ~nrow(.) > 0) -> scores[[k]] # drop empty tibbles
}
View(scores)
for (i in 1:length(scores)){
(names = names(scores[[i]]))
}
for (i in 1:length(scores)){
(names = names(scores[[i]]))
print(names)
}
for (i in 1:length(scores)){
(names = name(scores[[i]]))
print(names)
}
for (i in 1:length(scores)){
names = names(scores[[i]])
for (n in 1:length(names)){
scores[[i]][[n]]$idno <- names[n]
}
print(names)
}
View(scores)
View(scores)
View(items)
for (i in 1:length(scores)){
names = names(scores[[i]])
for (n in 1:length(names)){
scores[[i]][[n]]$idno <- names[n]
scores[[i]][[n]] %>% merge(items, by = idno)
}
print(names)
}
for(k in 1:length(key)){
message(paste0("scoring for key word ",k, "/", length(key),
" (", key[k],")..."))
item_vars %>% pbapply::pblapply(. %>% vsm_score(. , query = key[k],
accuracy = accuracy)#,cl = cl
) %>%
purrr::keep(., ~nrow(.) > 0) -> scores[[k]] # drop empty tibbles
}
for (i in 1:length(scores)){
names = names(scores[[i]])
for (n in 1:length(names)){
scores[[i]][[n]]$idno <- names[n]
scores[[i]][[n]] %>% merge(items, by = "idno")
}
print(names)
}
View(scores)
for(k in 1:length(key)){
message(paste0("scoring for key word ",k, "/", length(key),
" (", key[k],")..."))
item_vars %>% pbapply::pblapply(. %>% vsm_score(. , query = key[k],
accuracy = accuracy)#,cl = cl
) %>%
purrr::keep(., ~nrow(.) > 0) -> scores[[k]] # drop empty tibbles
}
for (i in 1:length(scores)){
names = names(scores[[i]])
for (n in 1:length(names)){
scores[[i]][[n]]$idno <- names[n]
scores[[i]][[n]] %<>% merge(items, by = "idno")
}
}
View(scores)
for(k in 1:length(key)){
message(paste0("scoring for key word ",k, "/", length(key),
" (", key[k],")..."))
item_vars %>% pbapply::pblapply(. %>% vsm_score(. , query = key[k],
accuracy = accuracy)#,cl = cl
) %>%
purrr::keep(., ~nrow(.) > 0) -> scores[[k]] # drop empty tibbles
}
for (i in 1:length(scores)){
names = names(scores[[i]])
for (n in 1:length(names)){
scores[[i]][[n]]$idno <- names[n]
scores[[i]][[n]] %<>% merge(items, by = "idno")
rm(names)
}
}
for(k in 1:length(key)){
message(paste0("scoring for key word ",k, "/", length(key),
" (", key[k],")..."))
item_vars %>% pbapply::pblapply(. %>% vsm_score(. , query = key[k],
accuracy = accuracy)#,cl = cl
) %>%
purrr::keep(., ~nrow(.) > 0) -> scores[[k]] # drop empty tibbles
}
# merge scores with dataset info
for (i in 1:length(scores)){
names = names(scores[[i]])
for (n in 1:length(names)){
scores[[i]][[n]]$idno <- names[n]
scores[[i]][[n]] %<>% merge(items, by = "idno")
}
rm(names)
}
View(scores)
wanna_read <- readline(  # ask whether to print or not
prompt = "Search complete! Print results in the console? (type y for YES, n for NO):"
)
message(paste0(sum(sapply(scores[[k]], nrow)), " result(s) for --",
key[k], "-- in ", length(scores[[k]]),
" library item(s):"))
for (i in 1:length(scores[[k]])){
message(paste0("    ",names(scores[[k]])[[i]]))
for (v in 1:nrow(scores[[k]][[i]])){
message(paste0("        ", scores[[k]][[i]]$doc[v],
" (", scores[[k]][[i]]$text[v],") - ",
100*(round(scores[[k]][[i]]$score[v], digits = 2)),
"% match"))
}
message("    ")
}
for (i in 1:length(scores[[k]])){
message(paste0("    ",names(scores[[k]])[[i]]))
for (v in 1:nrow(scores[[k]][[i]])){
message(paste0("        ", scores[[k]][[i]]$var[v],
" (", scores[[k]][[i]]$labl[v],") - ",
100*(round(scores[[k]][[i]]$score[v], digits = 2)),
"% match"))
}
message("    ")
}
library(WBqueryR)
WBquery(key = c("latitude", "longitude"), country = c("nga"))
query <- WBquery(key = c("latitude", "longitude"), country = c("nga"))
View(query)
library(WBqueryR)
query <- WBquery(key = c("latitude", "longitude"), country = c("nga"))
library(WBqueryR)
query <- WBquery(key = c("latitude", "longitude"), country = c("nga"))
library(WBqueryR)
query <- WBquery(key = c("latitude", "longitude"), country = c("nga"))
View(query)
WBquery(key = c("latitude", "longitude"), country = c("nga","tza"))
WBquery(key = c("latitude", "longitude"), country = c("nga","tza")) -> query
library(WBqueryR)
library(tidyverse)
library(RSelenium)
library(netstat)
RSelenium::rs_driver_object <- rsDriver(
browser = "chrome",
verbose = FALSE,
port = netstat::free_port()
)
library(WBqueryR)
library(tidyverse)
library(RSelenium)
library(netstat)
RSelenium::rs_driver_object <- rsDriver(
browser = "chrome",
verbose = FALSE,
port = netstat::free_port()
)
