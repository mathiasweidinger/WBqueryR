require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized application: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble, cl = cl) # save output as a listed tibble
stopCluster(cl)
return(item_vars)
} # end of WBquery()
key <- c("longitude|latitude")
item_vars <- WBquery(key = c("longitude", "latitude"),
country = c("malawi", "nigeria", "ghana", "niger", "rwanda"),
collection = c("lsms", "afrobarometer"),
sort_by = c("rank"), sort_order = c("asc") )
View(item_vars)
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "",    # collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order =""      # c("asc","desc")
){
# Load the package required to read JSON files.
require(rjson)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
r <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(r)
# reshape to get content
r %<>% content %$% result %$% rows
# collect names of relevant studies
items <- as.data.frame(do.call(rbind, r)) %>% # collapse list
tibble %$% # build tibble
idno # extract item names
message("gathering codebooks; this might take a while...")
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble %>% # save output as a listed tibble
mutate(grepl = grepl(key, # search for key words in each tibble
labl,ignore.case = TRUE)) %>% # ignore cases
filter(.$grepl == TRUE), # drop variables that do not match keys
cl = cl) %>% # set parallel clusters, end  parallelized task //
keep(., ~nrow(.) > 0) # drop empty tibbles
stopCluster(cl) # switch off clusters
return(item_vars) # give output
} # end of WBquery()
item_vars <- WBquery(key = c("longitude", "latitude"),
country = c("malawi", "nigeria", "ghana", "niger", "rwanda"),
collection = c("lsms", "afrobarometer"),
sort_by = c("rank"), sort_order = c("asc") )
View(item_vars)
View(item_vars)
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "",    # collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order =""      # c("asc","desc")
){
# Load the package required to read JSON files.
require(rjson)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
r <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(r)
# reshape to get content
r %<>% content %$% result %$% rows
# collect names of relevant studies
items <- as.data.frame(do.call(rbind, r)) %>% # collapse list
tibble %$% # build tibble
idno # extract item names
message("gathering codebooks; this might take a while...")
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble %>% # save output as a listed tibble
mutate(grepl = grepl(key, # search for key words in each tibble
labl,ignore.case = TRUE)) %>% # ignore cases
filter(.$grepl == TRUE), # drop variables that do not match keys
cl = cl) %>% # set parallel clusters, end  parallelized task //
keep(., ~nrow(.) > 0) # drop empty tibbles
stopCluster(cl) # switch off clusters
return(item_vars) # give output
} # end of WBquery()
item_vars <- WBquery(key = c("aggregate", "consumption"),
country = c("malawi", "nigeria", "ghana", "niger", "rwanda"),
collection = c("lsms", "afrobarometer"),
sort_by = c("rank"), sort_order = c("asc"))
View(item_vars)
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "",    # collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order =""      # c("asc","desc")
){
# Load the package required to read JSON files.
require(rjson)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
r <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(r)
# reshape to get content
r %<>% content %$% result %$% rows
# collect names of relevant studies
items <- as.data.frame(do.call(rbind, r)) %>% # collapse list
tibble %$% # build tibble
idno # extract item names
message("gathering codebooks; this might take a while...")
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble %>% # save output as a listed tibble
mutate(grepl = grepl(key, # search for key words in each tibble
labl,ignore.case = TRUE)) %>% # ignore cases
filter(.$grepl == TRUE) %>%  # drop variables that do not match keys
select(., -grepl), # drop helper variable from tibbles
cl = cl) %>% # set parallel clusters, end  parallelized task //
keep(., ~nrow(.) > 0) # drop empty tibbles
stopCluster(cl) # switch off clusters
return(item_vars) # give output
} # end of WBquery()
item_vars <- WBquery(key = c("aggregate", "consumption"),
country = c("malawi", "nigeria", "ghana", "niger", "rwanda"),
collection = c("lsms", "afrobarometer"),
sort_by = c("rank"), sort_order = c("asc"))
View(item_vars)
view(item_vars$`MWI_2010-2016_IHPS_v03_M`)
name(item_vars$GHA_1998_GLSS_v02_M)
names(item_vars$GHA_1998_GLSS_v02_M)
item_vars %>%
pblapply(
message(paste0(., " contains the variable........ ",
.$labl, sep = "" )))
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "",    # collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order =""      # c("asc","desc")
){
# Load the package required to read JSON files.
require(rjson)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
r <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(r)
# reshape to get content
r %<>% content %$% result %$% rows
# collect names of relevant studies
items <- as.data.frame(do.call(rbind, r)) %>% # collapse list
tibble %$% # build tibble
idno # extract item names
message("gathering codebooks; this might take a while...")
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble %>% # save output as a listed tibble
mutate(grepl = grepl(key, # search for key words in each tibble
labl,ignore.case = TRUE)) %>% # ignore cases
filter(.$grepl == TRUE) %>%  # drop variables that do not match keys
select(., -grepl), # drop helper variable from tibbles
cl = cl) %>% # set parallel clusters, end of parallelized task
keep(., ~nrow(.) > 0) # drop empty tibbles
stopCluster(cl) # switch off clusters
return(item_vars) # give output
} # end of WBquery()
item_vars <- WBquery(key = c("aggregate", "consumption"),
country = c("malawi", "nigeria", "ghana", "niger", "rwanda"),
collection = c("lsms", "afrobarometer"),
sort_by = c("rank"), sort_order = c("asc"))
# initiate WBquery()
WBquery <- function(key = "",           # search keys
from = "",          # start year of data collection (Integer)
to = "",            # end year of data collection (Integer)
country = "",       # country name(s) or iso3 codes
collection = "",    # collection id
access = "",        # type of access rights
sort_by = "",       # c("rank","title","nation","year")
sort_order =""      # c("asc","desc")
){
# Load the package required to read JSON files.
require(rjson)
require(magrittr) # required for double and subset-pipe operators.
require(tidyverse) # Load the tidyverse suite of packages.
require(httr) # package to work with APIs
require(pbapply) # add a progress bar to lapply() calls
# require(future.apply) # parallelized apply functions
require(parallel) # distributed (parallel) computing
source("R/multi_join.R") # source custom function multi-join()
# Process search criteria
key <- ifelse(exists("key"),
paste(key, collapse = "|"), "")
from <- ifelse(exists("from"),
paste(from, collapse = ","), "")
to <- ifelse(exists("to"),paste(to, collapse = ","), "")
country <- ifelse(exists("country"),
paste(country, collapse = "|"), "")
collection <- ifelse(exists("collection"),
paste(collection, collapse = ","), "")
access <- ifelse(exists("access"),
paste(access, collapse = ","), "")
sort_by <- ifelse(exists("sort_by"),
paste(sort_by, collapse = ","), "")
sort_order <- ifelse(exists("sort_order"),
paste(sort_order, collapse = ","), "")
# first search for datasets in collections, countries and time frame
# get their codebooks
# set path for catalog search
path_cat <- "http://microdata.worldbank.org/index.php/api/catalog/search"
r <- GET(path_cat,
query = list(from = from,
to = to,
country = country,
inc_iso = TRUE, # include iso3 country codes
collection = collection,
dtype = access,
ps = 10000, # max out how many sets are included
sorty_by = sort_by,
sort_order = sort_order))
# check whether call was successful (200 = success)
status_code(r)
# reshape to get content
r %<>% content %$% result %$% rows
# collect names of relevant studies
items <- as.data.frame(do.call(rbind, r)) %>% # collapse list
tibble %$% # build tibble
idno # extract item names
message("gathering codebooks; this might take a while...")
# set number of active cores for parallelization
detectCores() %>% makeCluster() -> cl # detect cores and make clusters
# load required packages for each cluster
clusterEvalQ(cl, {
require(httr)
require(magrittr)
require(pbapply)
require(tidyverse)
source("R/multi_join.R")
})
# download codebooks
item_vars <- items
names(item_vars) <- unlist(items)
item_vars %<>% # start with list of codebooks
pblapply(. %>% # start parallelized task: for each item...
paste0("http://microdata.worldbank.org/index.php/api/catalog/",
.,
"/variables") %>% # paste URL path for API
GET %>% # get codebooks from API
content %$% # extract content from JSON response file
variables %>% # select column "variables"
multi_join(join_func = rbind) %>% # "multi-join" custom function
data.frame %>% tibble %>% # save output as a listed tibble
mutate(grepl = grepl(key, # search for key words in each tibble
labl,ignore.case = TRUE)) %>% # ignore cases
filter(.$grepl == TRUE) %>%  # drop variables that do not match keys
select(., -grepl), # drop helper variable from tibbles
cl = cl) %>% # set parallel clusters, end of parallelized task
keep(., ~nrow(.) > 0) # drop empty tibbles
stopCluster(cl) # switch off clusters
return(item_vars) # give output
} # end of WBquery()
item_vars <- WBquery(key = c("aggregate", "consumption"),
#country = c("malawi", "nigeria", "ghana", "niger", "rwanda"),
#collection = c("lsms", "afrobarometer"),
#sort_by = c("rank"), sort_order = c("asc")
)
item_vars <- WBquery(key = c("aggregate", "consumption"),
country = c("malawi", "nigeria", "ghana", "niger", "rwanda"),
collection = c("lsms", "afrobarometer"),
sort_by = c("rank"), sort_order = c("asc")
)
